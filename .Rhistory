usethis::create_package()
usethis::create_package(".")
# creates description and namespace files
usethis::use_description()
usethis::use_package_doc()
usethis::use_mit_license("Keith Jennings")
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
load("~/Desktop/github/rainOrSnowTools/R/sysdata.rda")
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
devtools::document()
library(devtools)
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
devtools::check()
use_readme_rmd()
use_readme_md()
devtools::build_readme()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
library(devtools)
devtools::document()
devtools::check()
devtools::build_readme()
library(devtools)
devtools::document()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
# Get the pipe
`%>%` <- dplyr::`%>%` # add dplyr pipe
################################################################################
# Script for processing HADS DCP metadata
################################################################################
# Get all locations (50 states + DC)
states = c(state.abb, "DC")
networks = "DCP" # use networks = c("COOP", "DCP") for daily NOAA COOP data too
# URL strings for station info by state
url_01 = "https://mesonet.agron.iastate.edu/sites/networks.php?network="
url_02 = "_"
url_03 = "&format=csv&nohtml=on"
# URL strings for var info by station by state
url_var_01 = "https://hads.ncep.noaa.gov/csv/"
url_var_02 = ".csv"
# Empty dataframes
hads_meta <- data.frame()
hads_var_meta <- data.frame()
# Loop through states
for(i in 1:length(states)){
# Loop through networks
for(j in 1:length(networks)){
tmp_url = paste0(url_01, states[i],
url_02, networks[j],
url_03)
hads_meta <- dplyr::bind_rows(hads_meta,
read.csv(tmp_url))
} # end networks loop
# Download and bind var info by state
tmp_url = paste0(url_var_01, states[i],
url_var_02)
hads_var_meta <- dplyr::bind_rows(hads_var_meta,
read.csv(tmp_url))
} # end states loop
# Name the var of interest (air temperature)
var = "TA"
# Filter to only stations that monitor that var
hads_var_meta <- hads_var_meta %>%
dplyr::filter(dplyr::if_any(pe1:pe25, ~ . == var))
# Trim the station metadata to only those that measure TA
# And remove known bad stations from prev work
stations_remove <- c("WRSV1", "XONC1", "DPHC1", "DKKC2", "CNLC1")
hads_meta <- hads_meta %>%
dplyr::filter(stid %in% hads_var_meta$nwsli) %>%
dplyr::filter(!(stid %in% stations_remove))
# Make a timezone table
# TODO: this assumes CONUS and standard time only
tz_table <- lutz::tz_list() %>%
dplyr::filter(zone %in% c("PST", "MST", "CST", "EST")) %>%
dplyr::mutate(timezone_lst = paste0("Etc/GMT+", (utc_offset_h * (-1))))
# Add timezone info
# Needed for local time to UTC conversions
hads_meta <- hads_meta %>%
dplyr::mutate(timezone = lutz::tz_lookup_coords(lat = lat,
lon = lon,
method = "accurate"))
# Join timezone info
hads_meta <- dplyr::left_join(hads_meta,
dplyr::select(tz_table,
timezone = tz_name,
timezone_lst),
by = "timezone")
# First access the metadata from NOAA
# https://www.ncei.noaa.gov/maps/lcd/
# go to mapping tool, use polygon selector
# drag over area of interest
# and then click "Download Station List"
lcd_meta <- read.csv("data-raw/lcd_station_metadata_conus.csv")
# Format date and filter to current stations only
lcd_meta <- lcd_meta %>%
dplyr::mutate(short_id = stringr::str_sub(STATION_ID, 6, 10),
END_DATE = as.Date(END_DATE)) %>%
dplyr::filter(END_DATE > as.Date("2023-06-12"))
# URL for finding station codes
baseURL = "https://www.ncei.noaa.gov/data/local-climatological-data/access/"
year = 2023
# Build URL
URL = paste0(baseURL, year)
# Get all links from URL
links_full <- rvest::read_html(URL) %>%
rvest::html_nodes("a") %>%
rvest::html_attr('href')
links <- links_full %>%
stringr::str_sub(7,11)
links
# Make data frame to store info
info <- data.frame()
# Loop through IDs to finding matching stations and URLS
for(j in 1:length(lcd_meta$short_id)){
# Check for partial match in station ID and URLs
link.match = stringr::str_detect(links, pattern = lcd_meta[j, "short_id"])
# Find n matches
matches = logical()
matches = which(link.match == TRUE)
# Skip or download
print(paste("Station id =", lcd_meta[j, "short_id"]))
if(length(matches) == 0){
print("No match found, going to next station")
# Extract info for station and data
tmp.info <- data.frame(short_id = lcd_meta[j, "short_id"],
id = NA,
name = lcd_meta[j, "STATION"],
n_matches = length(matches),
link = NA)
info <- dplyr::bind_rows(info, tmp.info)
} else if(length(matches) > 0){
print(paste(length(matches), "match(es) found"))
for(k in 1:length(matches)){
# Link to met data
met.link = paste0(URL, "/",
links_full[matches[k]])
# Extract info for station and data
tmp.info <- data.frame(short_id = lcd_meta[j, "short_id"],
id = substr(links_full[matches[k]], 1, 11),
name = lcd_meta[j, "STATION"],
n_matches = length(matches),
link = met.link)
info <- dplyr::bind_rows(info, tmp.info)
}
}
}
# Join the data
lcd_meta <- dplyr::left_join(lcd_meta,
info,
by = c("short_id", "STATION" = "name"))
# Add timezone info
# Needed for local time to UTC conversions
lcd_meta <- lcd_meta %>%
dplyr::mutate(timezone = lutz::tz_lookup_coords(lat = LATITUDE,
lon = LONGITUDE,
method = "accurate"))
# Join timezone info
lcd_meta <- dplyr::left_join(lcd_meta,
dplyr::select(tz_table,
timezone = tz_name,
timezone_lst),
by = "timezone")
# You can access current station info at
# https://wcc.sc.egov.usda.gov/nwcc/inventory
# But it's a GUI, so I've downloaded it already
wcc_meta <- read.csv("data-raw/nwcc_inventory.csv")
# Add a timezone column
wcc_meta <- wcc_meta %>%
dplyr::mutate(timezone_lst = paste0("Etc/GMT+", gmt_offset),
elev_m = elev*0.3048)
################################################################################
# Collate the metadata into a single dataframe
################################################################################
all_meta <-
dplyr::bind_rows(
hads_meta %>%
dplyr::select(name = station_name,
id = stid,
lat, lon, elev,
timezone_lst) %>%
dplyr::mutate(network = "hads"),
lcd_meta %>%
dplyr::select(name = STATION,
id,
lat = LATITUDE,
lon = LONGITUDE,
elev = ELEVATION_.M.,
timezone_lst) %>%
dplyr::mutate(network = "lcd"),
wcc_meta %>%
dplyr::select(name = site_name,
id = station.id,
lat, lon,
elev = elev_m,
timezone_lst,
network) %>%
dplyr::mutate(id = as.character(id)),
)
all_meta
usethis::use_data(hads_meta, lcd_meta, wcc_meta, all_meta,
internal = TRUE, overwrite = TRUE)
# Get the pipe
`%>%` <- dplyr::`%>%` # add dplyr pipe
get_ecoregion = function(level = level){
temp = tempfile()
eco = tempfile()
URL_conus = glue::glue("https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/us/us_eco_l{level}_state_boundaries.zip")
# Increase timeout
options(timeout = 100)
download.file(URL_conus, temp)
# Unzip the contents of the temp and save unzipped contents in 'eco'
unzip(zipfile = temp, exdir = eco)
# Read the shapefile
sf::read_sf(eco) %>%
sf::st_make_valid() %>%
rmapshaper::ms_simplify(keep_shapes = TRUE) %>%
sf::st_transform(crs = 4326)
}
temp_ak = tempfile()
eco_ak = tempfile()
URL_ak = "https://gaftp.epa.gov/EPADataCommons/ORD/Ecoregions/ak/ak_eco_l3.zip"
download.file(URL_ak, temp_ak)
unzip(zipfile = temp_ak, exdir = eco_ak)
ak_ecoregion = sf::read_sf(eco_ak) %>%
sf::st_make_valid() %>%
rmapshaper::ms_simplify(keep_shapes = TRUE) %>%
sf::st_transform(crs = 4326)
conus_ecoregion = get_ecoregion(4)
ecoregions_states = dplyr::bind_rows(conus_ecoregion, ak_ecoregion)
sysdata <- load("R/sysdata.rda")
save(list = c(sysdata, "ecoregions_states"), file = "R/sysdata.rda")
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
library(devtools)
devtools::document()
devtools::check()
devtools::install_github("anguswg-ucsb/hydrofabric3D")
usethis::use_testthat()
lutz::tz_list()
usethis::use_test("get_tz")
library(testthat)
usethis::use_test("get_tz2")
devtools::test()
devtools::test()
devtools::load_all()
lat = 34.0549
lon = 118.2426
rainOrSnowTools:::get_tz(lat_obs = lat, lon_obs = lon)
timezone
# Act
timezone = rainOrSnowTools:::get_tz(lat_obs = lat, lon_obs = lon)
# Assert
testthat::expect_equal(timezone, "Etc/GMT+-8")
# Assert
testthat::expect_equal(timezone, "Etc/GMT+-")
# Assert
testthat::expect_equal(timezone, "Etc/GMT+-8")
devtools::test()
devtools::test()
# Arrange
lat = TRUE
lon = FALSE
# Act
timezone = rainOrSnowTools:::get_tz(lat_obs = lat, lon_obs = lon)
# Act
testthat::expect_error(
rainOrSnowTools:::get_tz(lat_obs = lat, lon_obs = lon)
)
lat = 100000000000000
lon = 100000000000000
rainOrSnowTools:::get_tz(lat_obs = lat, lon_obs = lon)
usethis::use_test("construct_gpm_base_url")
date_of_interest = "2024-01-25T01:45:59.000Z"
product_version = "GPM_3IMERGHHL.06"
# list of available GPM IMERG product versions
versions = list(
"GPM_3IMERGHHL.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06",
"GPM_3IMERGHHE.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHE.06",
"GPM_3IMERGHHL.07" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.07",
"GPM_3IMERGHH.07"  = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHH.07"
)
versions[[product_version]]
base     <- versions[[product_version]]
"{base}/{year}/{julian}/"
# character to insert values into via glue::glue()
url_trim <- "{base}/{year}/{julian}/"
url_trim
# Extract necessary date components
julian  <- format(dateTime, "%j")
date_of_interest = "2024-01-25T01:45:59.000Z"
product_version = "GPM_3IMERGHHL.06"
dateTime = as.POSIXct(date_of_interest, format = "%Y-%m-%dT%H:%M:%OS",  tz = "UTC")
dateTime
# list of available GPM IMERG product versions
versions = list(
"GPM_3IMERGHHL.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06",
"GPM_3IMERGHHE.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHE.06",
"GPM_3IMERGHHL.07" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.07",
"GPM_3IMERGHH.07"  = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHH.07"
)
# throw an error if the product_version is not in the versions list
if(!product_version %in% names(versions)) {
stop("Invalid 'product_version' argument, must be one of:\n > ", paste(paste0("'", names(versions), "'"), collapse = "\n > "))
}
base     <- versions[[product_version]]
# Extract necessary date components
julian  <- format(dateTime, "%j")
year    <- format(dateTime, "%Y")
# character to insert values into via glue::glue()
url_trim <- "{base}/{year}/{julian}/"
url_trim
as.character(glue::glue(url_trim))
date_of_interest = "2024-01-25T01:45:59.000Z"
bad_product_versions = list(FALSE, "i love cats", "tests are good", list())
bad_product_versions
bad_product_versions = list(FALSE, "i love cats", "tests are good", list())
for (bad_product_version in bad_product_versions) {
testthat::expect_error(
construct_gpm_base_url("2024-01-25T01:45:59.000Z",  bad_product_version)
)
}
for (bad_product_version in bad_product_versions) {
testthat::expect_error(
construct_gpm_base_url("2024-01-25T01:45:59.000Z",  "GPM_3IMERGHHL.06")
)
}
usethis::use_test("get_state")
lat = 44.91
lon = -66.99
rainOrSnowTools:::get_state(lon, lat)
# list of available GPM IMERG product versions
versions = list(
"GPM_3IMERGHHL.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06",
"GPM_3IMERGHHE.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHE.06",
"GPM_3IMERGHHL.07" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.07",
"GPM_3IMERGHH.07"  = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHH.07"
)
for (version in versions) {
message(version)
# testthat::expect_error(
# construct_gpm_base_url("2024-01-25T01:45:59.000Z",  bad_product_version)
# )
}
version
names(version)
versions[version]
# list of available GPM IMERG product versions
versions = list(
"GPM_3IMERGHHL.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06",
"GPM_3IMERGHHE.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHE.06",
"GPM_3IMERGHHL.07" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.07",
"GPM_3IMERGHH.07"  = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHH.07"
)
for (version_name in names(versions)) {
# names(version)
versions[version_name]
message(version_name)
# testthat::expect_error(
# construct_gpm_base_url("2024-01-25T01:45:59.000Z",  bad_product_version)
# )
}
# names(version)
versions[version_name]
version = versions[version_name]
construct_gpm_base_url(version_name, "2024-01-25T01:45:59.000Z")
construct_gpm_base_url("2024-01-25T01:45:59.000Z", version_name)
base_url = construct_gpm_base_url("2024-01-25T01:45:59.000Z", version_name)
is.character(base_url)
testthat::expect_true(is.character(base_url))
versions[version_name]
version = versions[version_name]
version %in% base_url
version
base_url
tidyr::contains(version, base_url)
grepl(version, base_url)
# list of available GPM IMERG product versions
versions = list(
"GPM_3IMERGHHL.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06",
"GPM_3IMERGHHE.06" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHE.06",
"GPM_3IMERGHHL.07" = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.07",
"GPM_3IMERGHH.07"  = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHH.07"
)
for (version_name in names(versions)) {
version = versions[version_name]
base_url = construct_gpm_base_url("2024-01-25T01:45:59.000Z", version_name)
testthat::expect_true(grepl(version, base_url))
testthat::expect_true(is.character(base_url))
}
for (version_name in names(versions)) {
message(version_name)
version = versions[version_name]
base_url = construct_gpm_base_url("2024-01-25T01:45:59.000Z", version_name)
testthat::expect_true(grepl(version, base_url))
testthat::expect_true(is.character(base_url))
}
for (version_name in names(versions)) {
message(version_name)
version = versions[version_name]
base_url = construct_gpm_base_url("2025-01-25T01:45:59.000Z", version_name)
testthat::expect_true(grepl(version, base_url))
testthat::expect_true(is.character(base_url))
}
version_name = 'GPM_3IMERGHHL.06'
version_url = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06"
version_name = 'GPM_3IMERGHHL.06'
version_url = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06"
date_of_interest = "2024-06-02T01:45:59.000Z"
version_name = 'GPM_3IMERGHHL.06'
version_url = "https://gpm1.gesdisc.eosdis.nasa.gov/opendap/hyrax/GPM_L3/GPM_3IMERGHHL.06"
date_of_interest = "2024-06-02T01:45:59.000Z"
construct_gpm_base_url(date_of_interest, version_name)
usethis::use_data_raw("product_versions")
