# This workflow uses actions that are not certified by GitHub.
# They are provided by a third-party and are governed by
# separate terms of service, privacy policy, and support
# documentation.

# GitHub recommends pinning actions to a commit SHA.
# To get a newer version, you will need to update the SHA.
# You can also reference a tag or branch, but the action may change without warning.

name: Deploy Infrastructure

on:
  push:
    branches:
      - cicd_pipeline

jobs:
  deploy:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@0e613a0980cbf65ed5b322eb7a1e075d28913a83
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_KEY }}
          aws-region: ${{ secrets.AWS_DEFAULT_REGION }}
      - name: Source build_static_resources.sh
        run: source sh/setup_static_assets.sh ${{ secrets.AWS_ACCOUNT_NUMBER }} ${{ secrets.ECR_REPO_NAME }} ${{ secrets.AWS_REGION }} ${{ secrets.AWS_PROFILE }}

      - name: Source build_static_resources.sh
        run: source sh/build_static_resources.sh ${{ secrets.AWS_ACCOUNT_NUMBER }} ${{ secrets.OUTPUT_S3_BUCKET }} ${{ secrets.TFSTATE_BUCKET }} ${{ secrets.ECR_REPO_NAME }} ${{ secrets.AWS_REGION }} ${{ secrets.AWS_PROFILE }}

      - name: Package Lambdas
        run: sh/package_lambdas.sh

      - name: Setup Static Buckets
        run: |
          source sh/setup_static_buckets.sh ${{ secrets.AWS_ACCOUNT_NUMBER }} ${{ secrets.OUTPUT_S3_BUCKET }} ${{ secrets.TFSTATE_S3_BUCKET }} ${{ secrets.AWS_REGION }} ${{ secrets.AWS_PROFILE }} true

      - name: Create ECR Repo
        run: |
          source sh/create_ecr_repo.sh ${{ secrets.AWS_ACCOUNT_NUMBER }} ${{ secrets.ECR_REPO_NAME }} ${{ secrets.AWS_REGION }} ${{ secrets.AWS_PROFILE }} true

      - name: Package Lambdas
        run: |
          source sh/package_lambdas.sh ${{ github.workspace }} true

      - name: Export Environment Variables
        run: |
          source sh/export_env_vars.sh ${{ secrets.AWS_PROFILE }} ${{ github.sha }} true

      - name: Print Exported Environment Variables
        run: |
          echo "Exported Environment Variables:"
          printenv | grep TF_VAR

      - name: Print Environment Variables in $GITHUB_ENV
        run: |
          echo "Environment Variables in \$GITHUB_ENV:"
          cat $GITHUB_ENV

    #   - name: Terraform Init
    #     run: terraform init -backend-config="bucket=${{ secrets.TFSTATE_BUCKET }}" -backend-config="profile=${{ secrets.AWS_PROFILE }}" -backend-config="region=${{ secrets.AWS_REGION }}"

    #   - name: Terraform Plan
    #     id: plan
    #     env:
    #       aws_profile: ${{ secrets.TF_VAR_aws_profile }}
    #       aws_account_number: ${{ secrets.TF_VAR_aws_account_number }}
    #       aws_region: ${{ secrets.TF_VAR_aws_region }}
    #       airtable_secret_prefix: ${{ secrets.TF_VAR_airtable_secret_prefix }}
    #       airtable_base_id: ${{ secrets.TF_VAR_airtable_base_id }}
    #       airtable_table_id: ${{ secrets.TF_VAR_airtable_table_id }}
    #       airtable_api_token: ${{ secrets.TF_VAR_airtable_api_token }}
    #       airtable_to_sqs_lambda_zip_file_name: ${{ secrets.TF_VAR_airtable_to_sqs_lambda_zip_file_name }}
    #       airtable_to_sqs_lambda_function_name: ${{ secrets.TF_VAR_airtable_to_sqs_lambda_function_name }}
    #       stage_s3_to_prod_s3_lambda_zip_file_name: ${{ secrets.TF_VAR_stage_s3_to_prod_s3_lambda_zip_file_name }}
    #       stage_s3_to_prod_s3_lambda_function_name: ${{ secrets.TF_VAR_stage_s3_to_prod_s3_lambda_function_name }}
    #       sqs_consumer_lambda_function_name: ${{ secrets.TF_VAR_sqs_consumer_lambda_function_name }}
    #       mros_append_daily_data_lambda_zip_file_name: ${{ secrets.TF_VAR_mros_append_daily_data_lambda_zip_file_name }}
    #       mros_append_daily_data_lambda_function_name: ${{ secrets.TF_VAR_mros_append_daily_data_lambda_function_name }}
    #       insert_into_dynamodb_lambda_zip_file_name: ${{ secrets.TF_VAR_insert_into_dynamodb_lambda_zip_file_name }}
    #       insert_into_dynamodb_lambda_function_name: ${{ secrets.TF_VAR_insert_into_dynamodb_lambda_function_name }}
    #       dynamodb_table_name: ${{ secrets.TF_VAR_dynamodb_table_name }}
    #       sqs_queue_name: ${{ secrets.TF_VAR_sqs_queue_name }}
    #       sqs_stage_queue_name: ${{ secrets.TF_VAR_sqs_stage_queue_name }}
    #       sqs_prod_to_output_queue_name: ${{ secrets.TF_VAR_sqs_prod_to_output_queue_name }}
    #       lambda_bucket_name: ${{ secrets.TF_VAR_lambda_bucket_name }}
    #       airtable_s3_bucket_name: ${{ secrets.TF_VAR_airtable_s3_bucket_name }}
    #       staging_s3_bucket_name: ${{ secrets.TF_VAR_staging_s3_bucket_name }}
    #       prod_s3_bucket_name: ${{ secrets.TF_VAR_prod_s3_bucket_name }}
    #       nasa_data_user_env_var: ${{ secrets.TF_VAR_nasa_data_user_env_var }}
    #       nasa_data_password_env_var: ${{ secrets.TF_VAR_nasa_data_password_env_var }}
    #       mros_ecr_repo_name: ${{ secrets.TF_VAR_mros_ecr_repo_name }}
    #       mros_ecr_repo_url: ${{ secrets.TF_VAR_mros_ecr_repo_url }}
    #       sns_output_data_topic: ${{ secrets.TF_VAR_sns_output_data_topic }}
    #       eventbridge_cron_rule_name: ${{ secrets.TF_VAR_eventbridge_cron_rule_name }}
    #       output_s3_bucket_name: ${{ secrets.TF_VAR_output_s3_bucket_name }}
    #       output_s3_object_key: ${{ secrets.TF_VAR_output_s3_object_key }}
    #       tfstate_s3_bucket_name: ${{ secrets.TF_VAR_tfstate_s3_bucket_name }}
    #       tfstate_s3_object_key: ${{ secrets.TF_VAR_tfstate_s3_object_key }}
    #     run: |
    #       terraform plan \
    #         -var "aws_profile=${aws_profile}" \
    #         -var "aws_account_number=${aws_account_number}" \
    #         -var "aws_region=${aws_region}" \
    #         -var "airtable_secret_prefix=${airtable_secret_prefix}" \
    #         -var "airtable_base_id=${airtable_base_id}" \
    #         -var "airtable_table_id=${airtable_table_id}" \
    #         -var "airtable_api_token=${airtable_api_token}" \
    #         -var "airtable_to_sqs_lambda_zip_file_name=${airtable_to_sqs_lambda_zip_file_name}" \
    #         -var "airtable_to_sqs_lambda_function_name=${airtable_to_sqs_lambda_function_name}" \
    #         -var "stage_s3_to_prod_s3_lambda_zip_file_name=${stage_s3_to_prod_s3_lambda_zip_file_name}" \
    #         -var "stage_s3_to_prod_s3_lambda_function_name=${stage_s3_to_prod_s3_lambda_function_name}" \
    #         -var "sqs_consumer_lambda_function_name=${sqs_consumer_lambda_function_name}" \
    #         -var "mros_append_daily_data_lambda_zip_file_name=${mros_append_daily_data_lambda_zip_file_name}" \
    #         -var "mros_append_daily_data_lambda_function_name=${mros_append_daily_data_lambda_function_name}" \
    #         -var "insert_into_dynamodb_lambda_zip_file_name=${insert_into_dynamodb_lambda_zip_file_name}" \
    #         -var "insert_into_dynamodb_lambda_function_name=${insert_into_dynamodb_lambda_function_name}" \
    #         -var "dynamodb_table_name=${dynamodb_table_name}" \
    #         -var "sqs_queue_name=${sqs_queue_name}" \
    #         -var "sqs_stage_queue_name=${sqs_stage_queue_name}" \
    #         -var "sqs_prod_to_output_queue_name=${sqs_prod_to_output_queue_name}" \
    #         -var "lambda_bucket_name=${lambda_bucket_name}" \
    #         -var "airtable_s3_bucket_name=${airtable_s3_bucket_name}" \
    #         -var "staging_s3_bucket_name=${staging_s3_bucket_name}" \
    #         -var "prod_s3_bucket_name=${prod_s3_bucket_name}" \
    #         -var "nasa_data_user_env_var=${nasa_data_user_env_var}" \
    #         -var "nasa_data_password_env_var=${nasa_data_password_env_var}" \
    #         -var "mros_ecr_repo_name=${mros_ecr_repo_name}" \
    #         -var "mros_ecr_repo_url=${mros_ecr_repo_url}" \
    #         -var "sns_output_data_topic=${sns_output_data_topic}" \
    #         -var "eventbridge_cron_rule_name=${eventbridge_cron_rule_name}" \
    #         -var "output_s3_bucket_name=${output_s3_bucket_name}" \
    #         -var "output_s3_object_key=${output_s3_object_key}" \
    #         -var "tfstate_s3_bucket_name=${tfstate_s3_bucket_name}" \
    #         -var "tfstate_s3_object_key=${tfstate_s3_object_key}" \
    #     continue-on-error: true

    #   - name: Terraform Apply
    #     run: terraform apply -auto-approve
